{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# since keras_vggface is not available on Kaggle, install it directly from Github\n!pip install git+https://github.com/rcmalli/keras-vggface.git","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport PIL\nimport keras\nimport keras_vggface\nfrom tqdm import tqdm\nfrom scipy.spatial import distance\nimport random","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read pairings data\ntrain_df = pd.read_csv(\"../input/recognizing-faces-in-the-wild/train_relationships.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_image(img, image_size):\n    '''\n    Take a PIL.Image object, resize to have shape (image_size, image_size), convert to grayscale and reduce\n    pixels' values to the range [0,1]. Returns a numpy ndarray.\n    '''\n    return np.asarray(img.resize((image_size, image_size), PIL.Image.ANTIALIAS).convert('L')) / 255.0\n\ndef preprocess_facenet(img, axis=-1):\n    '''\n    Preprocess images as required by Facenet. In particular, resize to 160x160x3.\n    '''\n    return np.asarray(img.resize((160, 160), PIL.Image.ANTIALIAS)) / 255.0\n    \ndef preprocess_vgg_face(img, version=2):\n    '''\n    Preprocess an image 'img' as required by VGGFace. In particular, call the dedicated function with\n    the correct version (version=2 is for 'resnet' implementation of the net).\n    '''\n    return keras_vggface.utils.preprocess_input(np.asarray(img).astype(np.float), version=version)\n\ndef l2_normalization(image, epsilon=1e-10):\n    '''\n    Performs L2 normalization on a set of input images 'image'. Namely, we divide by the square root\n    of the squared image, while taking the maximum with a small 'epsilon' ensures we avoid division by 0.\n    '''\n    return image / np.sqrt(np.maximum(np.sum(np.square(image), axis=-1, keepdims=True), epsilon))\n\ndef l2(x, y):\n    '''\n    Return the l2-norm distance among images x and y.\n    '''\n    return np.linalg.norm(x - y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_train_metadata(base_path):\n    # create a pandas DataFrame having the following columns. It will store useful information about each image\n    data = pd.DataFrame(columns=[\"ID\", \"FamilyID\", \"PersonID\", \"PictureID\", \"Image\"])\n    num = 0\n    # iterate over directory tree, and add each picture (preprocessed) and its metadata to the dataframe\n    for family in os.listdir(base_path):\n        for person in os.listdir(base_path + family):\n            image_num = 0  # the number of pictures for person. We need this because the numbering in the dataset is sort of odd\n            # there are some empty folders in the training set. This loop never executes if we encounter one of them\n            for picture in os.listdir(base_path + family + \"/\" + person):\n                # add a new entry to the dataframe\n                data.loc[num] = pd.Series({\"ID\": family + \"/\" + person, \"FamilyID\": family, \"PersonID\": person, \"PictureID\": image_num, \"Image\": num})          \n                image_num += 1\n                num += 1\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_train_images(base_path, preprocessor, num_images=12379, image_size=224):\n    # create a pandas DataFrame having the following columns. It will store useful information about each image\n    data = pd.DataFrame(columns=[\"ID\", \"FamilyID\", \"PersonID\", \"PictureID\", \"Image\"])\n    images = np.zeros((num_images, image_size, image_size, 3)) # pre-allocate array of images\n    num = 0\n    # iterate over directory tree, and add each picture (preprocessed) and its metadata to the dataframe\n    for family in os.listdir(base_path):\n        for person in os.listdir(base_path + family):\n            image_num = 0  # the number of pictures for person. We need this because the numbering in the dataset is sort of odd\n            # there are some empty folders in the training set. This loop never executes if we encounter one of them\n            for picture in os.listdir(base_path + family + \"/\" + person):\n                path = base_path + family + \"/\" + person + \"/\" + picture\n                image = PIL.Image.open(path)\n                # add a new entry to the dataframe\n                data.loc[num] = pd.Series({\"ID\": family + \"/\" + person, \"FamilyID\": family, \"PersonID\": person, \"PictureID\": image_num, \"Image\": num})          \n                # insert a new image in the array, after preprocessing\n                images[num, :, :, :] = preprocessor(image)\n                image_num += 1\n                num += 1\n    return data, images","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_test_images(base_path, image_size, num_images=6282):\n    # pre-allocate array of test images\n    images = np.zeros((num_images, image_size, image_size, 3))\n    num = 0\n    # open and insert in the array one preprocessed image after the other\n    for person in os.listdir(base_path):\n        images[num, :, :, :] = preprocess_facenet(PIL.Image.open(base_path + person))\n        num += 1\n    return images","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = load_train_metadata(\"../input/recognizing-faces-in-the-wild/train/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#_, train_images = load_train_images(\"../input/recognizing-faces-in-the-wild/train/\", preprocess_vgg_face)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data.set_index(\"ID\") # computationally efficient for the following task\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# build a dictionary mapping each unique person ID to the list of kins, as given by train_df\n#relations = dict()\n#for pair in train_df.index:\n#    elem = train_df.loc[pair, \"p1\"]\n#    if elem in relations:\n#        relations[elem].append(train_df.loc[pair, \"p2\"])\n#    else:\n#        relations[elem] = [train_df.loc[pair, \"p2\"]]\n\n# create a new column in the main dataframe for the kins. Notice some persons will have NaN\n#train_data[\"Relations\"] = pd.Series(relations)\n#del relations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# retrieve the number of unique families\nlen(train_data[\"FamilyID\"].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# retrieve the total number of pictures\ntrain_data.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# retrieve all unique persons\nlen(train_data.index.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the Facenet model for Keras, as implemented at https://github.com/nyoki-mtl/keras-facenet\n#model = keras.models.load_model(\"../input/facenet-keras/facenet_keras.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def embed_facenet(model, images, batch_size):\n    '''\n    Compute predictions (embeddings) for a set of images 'images', assuming 'model' is the Facenet\n    model that can be found at https://github.com/nyoki-mtl/keras-facenet. Predictions is unrolled\n    into batches of size 'batch_size'. Choosing a batch size too large will likely cause the kernel \n    to die.\n    '''\n    embedding_size = 128 # hand-recovered size of the Facenet output\n    num_images = np.shape(images)[0]\n    # pre-allocate array of image embeddings\n    embeddings = np.zeros((num_images, embedding_size))\n    start = 0\n    stop = batch_size\n    # loop over the batches and process them, using 'start' and 'stop' as offsets\n    while stop <= num_images:\n        print(\"Processing next batch...\")\n        embeddings[start:stop, :] = model.predict_on_batch(images[start:stop, :, :, :])\n        start += batch_size\n        stop += batch_size\n    # we still miss the last (possibly, incomplete) batch\n    embeddings[start:, :] = model.predict_on_batch(images[start:, :, :, :])\n    return embeddings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#help(keras_vggface.vggface.VGGFace)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###\n### THERE SEEM TO BE SPURIOUS ENTRIES IN TRAIN_DF, THAT IS PAIRS WITH NON-EXISTENT PEOPLE\n###","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# condense in a list all the pairs contained in train_df. Take into account that not all paths appearing\n# in train_df actually correspond to some training image\npairs = [(train_df.loc[idx, \"p1\"], train_df.loc[idx, \"p2\"]) for idx in train_df.index \n         if train_df.loc[idx, \"p1\"] in train_data.index and train_df.loc[idx, \"p2\"] in train_data.index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_train_image(image_path, preprocessor):\n    '''\n    Return a Numpy array of the image found at 'image_path' among the training images. Use\n    the 'preprocessor' to preprocess the image as required. If multiple photos for the same\n    individual are found, return a randomly picked one.\n    '''\n    base_path = \"../input/recognizing-faces-in-the-wild/train/\" + image_path\n    image = PIL.Image.open(base_path + \"/\" + random.choice(os.listdir(base_path)))\n    return preprocessor(image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sample_pairs(num, pairs, data):\n    '''\n    Return a sample of paired images. The sampling is done in such a way that positive and negative\n    pairs are in equal proportion. This function requires the number of couples to return, a ground-truth\n    list for the positive pairs and the images metadata.\n    '''\n    couples = list() # the pairs sampled so far\n    # sample num//2 positive pairs\n    while len(couples) < num//2:\n        couple = random.sample(pairs, k=1)[0] # sample a pair\n        couples.append(couple)\n    # sample (num - num//2) negative pairs \n    while len(couples) < num:\n        img1, img2 = random.sample(list(data.index), 2) # sample two images from the general set\n        # add them only if they are no-kin\n        if (img1, img2) not in pairs and (img2, img1) not in pairs:\n            couples.append((img1, img2))\n    return couples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_distances(couples, data, predictor, distance, preprocessor):\n    '''\n    Compute a distance metric among embeddings generated from a sample of images. Specifically,\n    this function returns a Pandas DataFrame with a 'distance' column for the metric computed on\n    the random pairs, and a 'label' column filled with 0 (no-kin) or 1 (kin). This function requires the\n    pairs to sample, the metadata of the images, the predictor function of a model to use as feature \n    extractor, a distance function to be computed and a preprocessor to be applied on the images (since they\n    are opened and preprocessed \"on-the-fly\".\n    '''\n    distances = list()\n    num = len(couples)\n    labels = [1 if idx <= num//2 else 0 for idx in range(num)] # we already know we will sample the first num//2 from the positives\n    # loop over the pairs of images and compute the distance on their embeddings\n    for img1, img2 in tqdm(couples, total=num):\n        # compute embeddings. Of course, since some images might appear in more than one pair, we are doing\n        # unnecessary work by loading and recomputing everytime their embeddings. Nevertheless, in order to \n        # avoid it we would need to allocate a quite large buffer of memory, clinching from a compute-bound \n        # workload to a memory-bound one, which is what we want to avoid. We also need to take into account\n        # that an image is actually very unlikely to appear more than once.\n        image1 = get_train_image(img1, preprocessor)\n        image2 = get_train_image(img2, preprocessor)\n        embeds = predictor(np.array([image1, image2]))\n        # compute the distance\n        distances.append(distance(embeds[0, :], embeds[1, :]))\n    return pd.DataFrame({\"distance\": distances, \"label\": labels})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the VGGFace model\nvgg = keras_vggface.VGGFace(include_top=False, model=\"resnet50\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# freeze the layers\nfor layer in vgg.layers:\n    layer.trainable = True\nfor layer in vgg.layers[:-3]:\n    layer.trainable = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dist_df = compute_distances(2000, pairs, train_images, train_data, model, distance.euclidean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import style\n\nstyle.use(\"ggplot\")\nsample_size = 1000\n\ndistances = {\"euclidean\": distance.euclidean, \"cosine\": distance.cosine, \"l2\": l2}\nf, axes = plt.subplots(nrows=1, ncols=len(distances), figsize=(20,5))\nf.suptitle(\"distances among image pairs\", fontsize=20)\ni = 0\n# sample pairs, to be reused for each distance metric for consistency\nsample = sample_pairs(sample_size, pairs, train_data)\n# for each distance in 'distances', compute it among the pairs and plot a scatterplot\nfor ax in axes.flatten():\n    dist = compute_distances(sample, train_data, model.predict_on_batch, distances[list(distances.keys())[i]], preprocess_facenet)\n    # shuffle the dataframe, otherwise it is perfectly split between kin and no-kin\n    dist = dist.sample(frac=1)\n    ax.set_title(list(distances.keys())[i])\n    plot = sns.scatterplot(list(range(len(dist))), dist[\"distance\"], hue=dist[\"label\"], ax=ax)\n    # plot a horizontal line at an 'ideal' boundary, handcrafted\n    i += 1\ndel dist\ndel sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample a given number of families to be used as hold-out validation set\n#val = np.random.choice(train_data[\"FamilyID\"], size=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# include in the validation set all the pairs corresponding to the sampled families, making sure the individuals\n# appear among the training images\n#val = [(train_df.loc[idx, \"p1\"], train_df.loc[idx, \"p2\"]) for idx in train_df.index \n#       if str(train_df.loc[idx, \"p1\"]).split(\"/\")[0] in val and str(train_df.loc[idx, \"p2\"]).split(\"/\")[0] in val\n#      and train_df.loc[idx, \"p1\"] in train_data.index and train_df.loc[idx, \"p2\"] in train_data.index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# update the list of training pairs removing those destined to the validation set\n#pairs = [couple for couple in pairs if str(train_data.loc[couple[0], \"FamilyID\"]) not in val and \n#         str(train_data.loc[couple[1], \"FamilyID\"]) not in val] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fakeGen(pairs, batch_size, data, preprocessor=preprocess_vgg_face):\n    '''\n    Temporary generator function. Works like 'sample_pairs', but the output conforms to the\n    Keras 'fit_generator' routine for a siamese network, which requires a couple of inputs and\n    their label.\n    '''\n    a = list() # the pairs sampled so far\n    b = list()\n    y = list()\n    # sample num//2 positive pairs\n    while len(a) < batch_size//2:\n        couple = random.sample(pairs, k=1)[0] # sample a pair\n        a.append(get_train_image(couple[0], preprocessor))\n        b.append(get_train_image(couple[1], preprocessor))\n        y.append(1)\n    # sample (num - num//2) negative pairs \n    while len(a) < batch_size:\n        img1, img2 = random.sample(list(data.index), 2) # sample two images from the general set\n        # add them only if they are no-kin\n        if (img1, img2) not in pairs and (img2, img1) not in pairs:\n            a.append(get_train_image(img1, preprocessor))\n            b.append(get_train_image(img2, preprocessor))\n            y.append(0)\n    # convert from list to ndarray\n    a = np.array(a)\n    b = np.array(b)\n    y = np.array(y)\n    # shuffle the three arrays. To make sure the relative order of the pairs and labels is kept,\n    # we retrieve the current random number generator and reset it everytime. If we did not shuffle\n    # the batches, the network would always see batch_size//2 positive pairs and then (batch_size - \n    # batch_size//2) negative pairs, making the learning sort of rough\n    rng = np.random.get_state()\n    np.random.shuffle(a)\n    np.random.set_state(rng)\n    np.random.shuffle(b)\n    np.random.set_state(rng)\n    np.random.shuffle(y)\n    return [a, b], y  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fakefakeGen(pairs, batch_size, data):\n    while True:\n        yield fakeGen(pairs, batch_size, data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#help(keras.Model.fit_generator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_bias(shape):\n    return np.random.normal(loc=0.5, scale=1e-2,size=shape)\n  \ndef initialize_weights(shape):\n    return np.random.normal(loc=0.0, scale=1e-2, size=shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val = [pair for pair in pairs if \"F09\" in pair[0]]\npairs = [pair for pair in pairs if \"F09\" not in pair[0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(val), len(pairs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from keras.layers import Input, Dense, Dropout, Concatenate, GlobalMaxPool2D, GlobalAvgPool2D, Multiply, Lambda\nfrom keras import Model\nfrom keras.optimizers import Adam\nfrom keras.regularizers import l2\nimport keras.backend as K\nimport threading\n\ndimension = (224, 224, 3)\nbatch_size = 32\nepochs = 35\n\nleft_input = Input(dimension)\nright_input = Input(dimension)\n\nx1 = vgg(left_input)\nx2 = vgg(right_input)\nx3 = Concatenate(axis=-1)([GlobalMaxPool2D()(x1), GlobalAvgPool2D()(x1)])\nx4 = Concatenate(axis=-1)([GlobalMaxPool2D()(x2), GlobalAvgPool2D()(x2)])\nfc = Dense(100, activation=\"relu\", kernel_regularizer=l2(1e-3),kernel_initializer=initialize_weights,\n           bias_initializer=initialize_bias)\nx5 = fc(x3)\nx6 = fc(x4)\nx7 = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))([x5, x6])\n# |h1-h2|^2\nx8 = Lambda(lambda tensor: K.square(tensor))(x7)\n# h1*h2\nx9 = Multiply()([x5, x6])\n# |h1-h2|^2 + h1*h2\nx10 = Concatenate(axis=-1)([x8,x9])\n\nx11 = Dense(100,activation='relu',kernel_regularizer=l2(1e-3), kernel_initializer=initialize_weights,\n            bias_initializer=initialize_bias)(x10)\nx12 = Dropout(0.2)(x11)\n\nprediction = Dense(1, activation=\"sigmoid\", bias_initializer=initialize_bias)(x10)\nsiamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\n\n# decreasing the learning rate by a factor of 100 reduced a lot the loss on the very first batch. Alos choosing a more suitable\n# weight initialization seemed to produce benefits\nsiamese_net.compile(optimizer=Adam(1e-5), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\nsiamese_net.fit_generator(fakefakeGen(pairs, batch_size, train_data), epochs=epochs, steps_per_epoch=(len(pairs) * 2)/batch_size, \n                          validation_steps=(len(val) * 2)/batch_size, validation_data=fakefakeGen(val, batch_size, train_data), \n                          use_multiprocessing=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The two following cells owe a lot to 'adityajn' user. Thanks a lot!\n\nsubmission = pd.read_csv('../input/recognizing-faces-in-the-wild/sample_submission.csv')\nsubmission['p1'] = submission.img_pair.apply( lambda x: '../input/test/'+x.split('-')[0] )\nsubmission['p2'] = submission.img_pair.apply( lambda x: '../input/test/'+x.split('-')[1] )\nprint(submission.shape)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = list()\nfor i,j in tqdm([ (0,500),(500,1000),(1000,1500),(1500,2000),(2000,2500),\n                 (2500,3000),(3000,3500),(3500,4000),(4000,4500),(4500,5000),(5000,5310) ]):\n    imgs1 = np.array( [ read_img(photo) for photo in submission.p1.values[i:j] ] )\n    imgs2 = np.array( [ read_img(photo) for photo in submission.p2.values[i:j] ] )\n    prob =  siamese_net.predict( [ imgs1, imgs2 ] )\n    probs.append(np.squeeze(prob))\n    del imgs1,imgs2\n    gc.collect()\n\nsubmission.is_related = np.concatenate(probs)\nsubmission.drop( ['p1','p2'],axis=1,inplace=True )\nsubmission.head()\nsubmission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_im = os.listdir(\"../input/recognizing-faces-in-the-wild/test/\")\n#test_df = pd.read_csv(\"../input/recognizing-faces-in-the-wild/sample_submission.csv\")\n#test_df[\"distance\"] = 0\n#img2idx = dict()\n#for idx, img in enumerate(test_im):\n#    img2idx[img] = idx\n#for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n#    imgs = [norm[img2idx[img]] for img in row.img_pair.split(\"-\")]\n#    test_df.loc[idx, \"distance\"] = distance.euclidean(*imgs)\n#all_distances = test_df.distance.values\n#sum_dist = np.sum(all_distances)\n#probs = []\n#for dist in tqdm(all_distances):\n#    prob = np.sum(all_distances[np.where(all_distances <= dist)[0]]) / sum_dist\n#    probs.append(1 - prob)\n#sub_df = pd.read_csv(\"../input/recognizing-faces-in-the-wild/sample_submission.csv\").copy()\n#sub_df.is_related = probs\n#sub_df.to_csv(\"../input/submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sub_df.head(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test = np.load(\"train_embeddings.npy\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cross_validation(data, pairs, num_folds):\n    '''\n    Perform the cross-validation strategy as in \"Visual Kinship Recognition of Families in the Wild\" (Robinson, 2017). \n    Given the training dataset 'data', and the dataframe of pairings 'pairs', split the former into\n    num_folds foldings. Returns the indices of the foldings in a dictionary format, for example:\n    {0: [1, 23, 5, 2], 1: [7, 9, 0], ...}; basically, the keys should be the fold numbers, while the values\n    should be a list (or ndarray) of the corresponding indexes (to be used on the training dataset). \n    '''\n    ans = dict()\n    ## TODO: define the cross-validation folds ##\n    # make sure each family does not overlap over folds, i.e. each family appears entirely in one and only one fold\n    # randomly mismatch pairs in each fold until the number of negative and positive pairs are the same in each fold\n    # (i.e., negative pairs are added at random until it makes up 50% of the respective fold). Thus, the total number \n    # of positive and negative labels are equivalent.\n    return ans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One kernel generates an embedding for each image using a modified version of VGG, sort of VGG-Facenet, and then computes\n# the cosine similarity and compresses everything to output a probability of being of the same family. What about computational\n# budget? Also seems to have fragile mathematical foundations.\n# Try to see if can apply transfer learning, i.e. a similar problem has already been tackled in a related domain (i.e. speech recognition)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}